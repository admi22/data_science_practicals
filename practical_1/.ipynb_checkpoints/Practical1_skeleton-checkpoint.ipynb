{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "id1w7kwZVF3I"
   },
   "source": [
    "# Practical 1 : Implementation of Linear Regression (Ridge, Lasso)\n",
    "\n",
    "- Train a linear model\n",
    "    - using least squares method\n",
    "    - implement the model from scratch using NumPy \n",
    "    - Use learning curves plot to understand whether the linear moel is overfitting or underfitting\n",
    "- Train linear models with \n",
    "    - regularization (Ridge and Lasso)\n",
    "    - polynomial basis expansion\n",
    "    - use validation data to choose the hyperparameters\n",
    "    - scikit-learn\n",
    "    - Optional task: Use k-fold cross validation to choose the optimal hyperparameters (5 bonus points)\n",
    "\n",
    "We will use the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CTZv9o5i4gy3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import _pickle as cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b1-ZQWqTVPno"
   },
   "source": [
    "We will use the winequality dataset for this practical. The dataset is available here:\n",
    "https://archive.ics.uci.edu/ml/datasets/Wine+Quality. \n",
    "In order to make it easier to import the dataset, the dataset has been converted to the numpy array format and shuffled, so that we can start the practical directly. The converted dataset is available on the OLAT page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TzDL9RQiVaPY"
   },
   "source": [
    "The dataset has two files. We’ll focus on the white wine data, which is the larger dataset. The following code loads the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1423,
     "status": "ok",
     "timestamp": 1596436129238,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "NYkwbebUVO_i",
    "outputId": "80ed8916-85c3-4564-cda8-d8a8f36aaa1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X is a (4898, 11) matrix, which contains 4898 data records and 11 features.\n",
      "y is a 4898-dimentional vector, which stores the corresponding labels of the data records in X\n"
     ]
    }
   ],
   "source": [
    "# load the white wine dataset\n",
    "# X is the feature matrix that stores the feature values of the data records\n",
    "# y is the label vector that stores the labels of the data records\n",
    "X, y = cp.load(open('winequality-white.pickle', 'rb'))\n",
    "\n",
    "# check the size of the data\n",
    "print(\"X is a {} matrix, which contains {} data records and {} features.\".format(X.shape, X.shape[0], X.shape[1]))\n",
    "print(\"y is a {}-dimentional vector, which stores the corresponding labels of the data records in X\".format(y.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CGuNg0KbWN0z"
   },
   "source": [
    "We next split the data into training data and test data. \n",
    "In practice, we should sample randomly 80% of the data as training data and the rest as the test data. . \n",
    "However, in order to get consistent results, we use the first 80% of the data as training\n",
    "data and the remaining as the test data. \n",
    "To achieve this split, we define the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1416,
     "status": "ok",
     "timestamp": 1596436129239,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "6ZqbBa8bWNYg",
    "outputId": "da274c4e-c3ed-4ac0-8442-27befcf26f4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (3918, 11)\n",
      "Shape of y_train: (3918,)\n",
      "Shape of X_test: (980, 11)\n",
      "Shape of y_test: (980,)\n"
     ]
    }
   ],
   "source": [
    "# The function splits the dataset into training data and testing data.\n",
    "# The parameter split_coeff is a percentage value such that\n",
    "# the first split_coeff of the dataset goes to the training dataset \n",
    "# and the remaining data goes to the test dataset.\n",
    "def split_data(X, y, split_coeff):\n",
    "    N, _ = X.shape # get the number of records (rows)\n",
    "    train_size = int(split_coeff * N) # use the first split_coeff of the data as the training data\n",
    "    X_train = X[:train_size] # the first training_size records\n",
    "    y_train = y[:train_size]\n",
    "    X_test = X[train_size:] # the last test_size records\n",
    "    y_test = y[train_size:]\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = split_data(X, y, 0.8) # split the data with split_coeff=0.8\n",
    "\n",
    "# check the size of the splitted dataset\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q2yKNR49Wkn8"
   },
   "source": [
    "## Understanding What We’re Predicting\n",
    "\n",
    "Let’s first check\n",
    "the distribution of the y-values in the training data. \n",
    "You will find that the values are integers between 3 and 9 indicating the quality of the wine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-PMpsZNSWthB"
   },
   "source": [
    "### **Task 1**\n",
    "Make a bar chart showing the distribution of y-values in the training data. You will find that the y-values are integers from 3 to 9, which indicate the quality of the wines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1409,
     "status": "ok",
     "timestamp": 1596436129240,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "4L_JDK3dWrsR",
    "outputId": "71b22bf6-77ce-4bd6-d5b1-61f633923144"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAARCklEQVR4nO3dfYxldX3H8fenu0oVpWCZEtzFLpqFBklddIK0VkNLlSfjYv+wS1JBS1yN0Go1MUv7B8aGhraiLanFrLIFUlxKQcKm4MNKjaSJKANuYHkqAywy24UdSwtWDRX49o85W+4u+zAz9869C7/3K7mZc7/nd8753pvsZ878zrl3U1VIktrwC6NuQJI0PIa+JDXE0Jekhhj6ktQQQ1+SGrJ41A3sy6GHHlrLli0bdRuS9KJx++23/6iqxna3br8P/WXLljExMTHqNiTpRSPJI3ta5/SOJDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSH7DP0k65JsT7K5p/ZPSTZ1jy1JNnX1ZUl+1rPuiz3bvCXJXUkmk1ySJAvyiiRJezSbT+ReDvwdcOWOQlX9/o7lJBcDT/aMf7CqVuxmP5cCHwK+B9wEnAJ8bc4dSy8hy9bcOOoWANhy0emjbkFDss8z/aq6BXhid+u6s/X3Aev3to8khwMHVdWtNfNfdV0JnDHnbiVJfel3Tv/twONV9UBP7cgkP0jynSRv72pLgKmeMVNdbbeSrE4ykWRienq6zxYlSTv0G/pnsvNZ/jbgdVV1HPAJ4CtJDprrTqtqbVWNV9X42NhuvyhOkjQP8/6WzSSLgd8D3rKjVlVPA093y7cneRA4CtgKLO3ZfGlXkyQNUT9n+r8L3FdV/z9tk2QsyaJu+fXAcuChqtoGPJXkhO46wFnADX0cW5I0D7O5ZXM98F3g6CRTSc7pVq3ihRdw3wHc2d3CeS3wkaracRH4o8CXgUngQbxzR5KGbp/TO1V15h7qH9hN7Trguj2MnwCOnWN/kqQB8hO5ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqyD5DP8m6JNuTbO6pfTrJ1iSbusdpPevOTzKZ5P4kJ/fUT+lqk0nWDP6lSJL2ZTZn+pcDp+ym/vmqWtE9bgJIcgywCnhjt83fJ1mUZBHwBeBU4BjgzG6sJGmIFu9rQFXdkmTZLPe3Eri6qp4GHk4yCRzfrZusqocAklzdjb1n7i1Lkuarnzn985Lc2U3/HNLVlgCP9oyZ6mp7qkuShmi+oX8p8AZgBbANuHhQDQEkWZ1kIsnE9PT0IHctSU2bV+hX1eNV9WxVPQd8ieencLYCR/QMXdrV9lTf0/7XVtV4VY2PjY3Np0VJ0m7MK/STHN7z9L3Ajjt7NgCrkhyQ5EhgOfB94DZgeZIjk7ycmYu9G+bftiRpPvZ5ITfJeuBE4NAkU8AFwIlJVgAFbAE+DFBVdye5hpkLtM8A51bVs91+zgO+ASwC1lXV3YN+MZKkvZvN3Ttn7qZ82V7GXwhcuJv6TcBNc+pOkjRQfiJXkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSH7DP0k65JsT7K5p/bXSe5LcmeS65Mc3NWXJflZkk3d44s927wlyV1JJpNckiQL8ookSXs0mzP9y4FTdqltBI6tql8H/h04v2fdg1W1ont8pKd+KfAhYHn32HWfkqQFts/Qr6pbgCd2qX2zqp7pnt4KLN3bPpIcDhxUVbdWVQFXAmfMq2NJ0rwNYk7/D4Gv9Tw/MskPknwnydu72hJgqmfMVFfbrSSrk0wkmZienh5Ai5Ik6DP0k/wZ8AxwVVfaBryuqo4DPgF8JclBc91vVa2tqvGqGh8bG+unRUlSj8Xz3TDJB4B3Ayd1UzZU1dPA093y7UkeBI4CtrLzFNDSriZJGqJ5neknOQX4FPCeqvppT30syaJu+fXMXLB9qKq2AU8lOaG7a+cs4Ia+u5ckzck+z/STrAdOBA5NMgVcwMzdOgcAG7s7L2/t7tR5B/CZJD8HngM+UlU7LgJ/lJk7gV7BzDWA3usAkqQh2GfoV9WZuylftoex1wHX7WHdBHDsnLqTJA2Un8iVpIYY+pLUEENfkhpi6EtSQ+Z9n770YrZszY2jbkEaCc/0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JasisQj/JuiTbk2zuqb0mycYkD3Q/D+nqSXJJkskkdyZ5c882Z3fjH0hy9uBfjiRpb2Z7pn85cMoutTXAzVW1HLi5ew5wKrC8e6wGLoWZXxLABcBbgeOBC3b8opAkDcesQr+qbgGe2KW8EriiW74COKOnfmXNuBU4OMnhwMnAxqp6oqr+C9jIC3+RSJIWUD9z+odV1bZu+THgsG55CfBoz7iprran+gskWZ1kIsnE9PR0Hy1KknoN5EJuVRVQg9hXt7+1VTVeVeNjY2OD2q0kNa+f0H+8m7ah+7m9q28FjugZt7Sr7akuSRqSfkJ/A7DjDpyzgRt66md1d/GcADzZTQN9A3hXkkO6C7jv6mqSpCFZPJtBSdYDJwKHJpli5i6ci4BrkpwDPAK8rxt+E3AaMAn8FPggQFU9keTPgdu6cZ+pql0vDkuSFtCsQr+qztzDqpN2M7aAc/ewn3XAull3J0kaKD+RK0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhsw79JMcnWRTz+OpJB9P8ukkW3vqp/Vsc36SyST3Jzl5MC9BkjRbi+e7YVXdD6wASLII2ApcD3wQ+HxVfbZ3fJJjgFXAG4HXAt9KclRVPTvfHiRJczOo6Z2TgAer6pG9jFkJXF1VT1fVw8AkcPyAji9JmoVBhf4qYH3P8/OS3JlkXZJDutoS4NGeMVNd7QWSrE4ykWRienp6QC1KkvoO/SQvB94D/HNXuhR4AzNTP9uAi+e6z6paW1XjVTU+NjbWb4uSpM4gzvRPBe6oqscBqurxqnq2qp4DvsTzUzhbgSN6tlva1SRJQzKI0D+TnqmdJIf3rHsvsLlb3gCsSnJAkiOB5cD3B3B8SdIszfvuHYAkBwLvBD7cU/6rJCuAArbsWFdVdye5BrgHeAY41zt3JGm4+gr9qvoJ8Mu71N6/l/EXAhf2c0xJ0vz5iVxJakhfZ/qSXhqWrblx1C0AsOWi00fdwkueZ/qS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhrSd+gn2ZLkriSbkkx0tdck2Zjkge7nIV09SS5JMpnkziRv7vf4kqTZG9SZ/m9X1YqqGu+erwFurqrlwM3dc4BTgeXdYzVw6YCOL0mahYWa3lkJXNEtXwGc0VO/smbcChyc5PAF6kGStItBhH4B30xye5LVXe2wqtrWLT8GHNYtLwEe7dl2qqvtJMnqJBNJJqanpwfQoiQJYPEA9vFbVbU1ya8AG5Pc17uyqipJzWWHVbUWWAswPj4+p20lSXvW95l+VW3tfm4HrgeOBx7fMW3T/dzeDd8KHNGz+dKuJkkagr5CP8mBSV69Yxl4F7AZ2ACc3Q07G7ihW94AnNXdxXMC8GTPNJAkaYH1O71zGHB9kh37+kpVfT3JbcA1Sc4BHgHe142/CTgNmAR+Cnywz+NLkuagr9CvqoeAN+2m/p/ASbupF3BuP8eUJM2fn8iVpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JD5h36SY5I8u0k9yS5O8nHuvqnk2xNsql7nNazzflJJpPcn+TkQbwASdLsLe5j22eAT1bVHUleDdyeZGO37vNV9dnewUmOAVYBbwReC3wryVFV9WwfPUiS5mDeZ/pVta2q7uiWfwzcCyzZyyYrgaur6umqehiYBI6f7/ElSXM3kDn9JMuA44DvdaXzktyZZF2SQ7raEuDRns2m2PsvCUnSgPUd+kleBVwHfLyqngIuBd4ArAC2ARfPY5+rk0wkmZienu63RUlSp6/QT/IyZgL/qqr6KkBVPV5Vz1bVc8CXeH4KZytwRM/mS7vaC1TV2qoar6rxsbGxflqUJPXo5+6dAJcB91bV53rqh/cMey+wuVveAKxKckCSI4HlwPfne3xJ0tz1c/fO24D3A3cl2dTV/hQ4M8kKoIAtwIcBquruJNcA9zBz58+53rkjScM179Cvqn8DsptVN+1lmwuBC+d7TElSf/xEriQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDennw1mSNFDL1tw46hbYctHpo25hQXmmL0kN8UxfQ7U/nMlJLfNMX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JasjQQz/JKUnuTzKZZM2wjy9JLRvqF64lWQR8AXgnMAXclmRDVd0zzD5a5BedSYLhf8vm8cBkVT0EkORqYCVg6EvaL+wvJ0gL9b3+ww79JcCjPc+ngLfuOijJamB19/R/ktw/hN4W0qHAj0bdxH7C92Jnvh878/3o5C/7ei9+dU8r9svv06+qtcDaUfcxKEkmqmp81H3sD3wvdub7sTPfj+ct1Hsx7Au5W4Ejep4v7WqSpCEYdujfBixPcmSSlwOrgA1D7kGSmjXU6Z2qeibJecA3gEXAuqq6e5g9jMhLZqpqAHwvdub7sTPfj+ctyHuRqlqI/UqS9kN+IleSGmLoS1JDDP0FkuSIJN9Ock+Su5N8bNQ97Q+SLErygyT/MupeRinJwUmuTXJfknuT/MaoexqlJH/S/TvZnGR9kl8cdU/DlGRdku1JNvfUXpNkY5IHup+HDOJYhv7CeQb4ZFUdA5wAnJvkmBH3tD/4GHDvqJvYD/wt8PWq+jXgTTT8niRZAvwxMF5VxzJzk8eq0XY1dJcDp+xSWwPcXFXLgZu7530z9BdIVW2rqju65R8z8496yWi7Gq0kS4HTgS+PupdRSvJLwDuAywCq6n+r6r9H2tToLQZekWQx8ErgP0bcz1BV1S3AE7uUVwJXdMtXAGcM4liG/hAkWQYcB3xvxK2M2t8AnwKeG3Efo3YkMA38QzfV9eUkB466qVGpqq3AZ4EfAtuAJ6vqm6Ptar9wWFVt65YfAw4bxE4N/QWW5FXAdcDHq+qpUfczKkneDWyvqttH3ct+YDHwZuDSqjoO+AkD+tP9xaibq17JzC/D1wIHJvmD0Xa1f6mZe+sHcn+9ob+AkryMmcC/qqq+Oup+RuxtwHuSbAGuBn4nyT+OtqWRmQKmqmrHX37XMvNLoFW/CzxcVdNV9XPgq8Bvjrin/cHjSQ4H6H5uH8RODf0FkiTMzNneW1WfG3U/o1ZV51fV0qpaxsxFun+tqibP5qrqMeDRJEd3pZNo++vFfwickOSV3b+bk2j4wnaPDcDZ3fLZwA2D2Kmhv3DeBryfmTPaTd3jtFE3pf3GHwFXJbkTWAH8xWjbGZ3uL55rgTuAu5jJpaa+jiHJeuC7wNFJppKcA1wEvDPJA8z8NXTRQI7l1zBIUjs805ekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSH/B/Xmfgs8s/C1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title\n",
    "# Task 1: \n",
    "# the function takes the y-values in the training data as the input and makes the bar chart. \n",
    "# Hint: Your function should make a bar chart looks like the bar chart below.\n",
    "def plot_bar_chart_score(y_train):\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    plt.hist(y_train, [1,2,3,4,5,6,7,8,9,10])\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "\n",
    "plot_bar_chart_score(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GxjlElni2FcH"
   },
   "source": [
    "### **Task 2** \n",
    "This task is to build a trivial predictor, which always returns the mean of the y-values of the training data. We consider the trivial model as a baseline. The linear regression models we build later should perform better than this trivial model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1402,
     "status": "ok",
     "timestamp": 1596436129240,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "-V3xFYexX1lt",
    "outputId": "5e57738e-87d5-408c-f1bf-9df66a175f35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of y on the training label values is 5.878764675855028\n"
     ]
    }
   ],
   "source": [
    "#@title\n",
    "# Task 2: implement the trivial predictor\n",
    "# The function computes the average value of y on the training label values\n",
    "def compute_average(y_train):\n",
    "    # The code below is just for compilation. \n",
    "    # You need to delete it and write your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    # Hint: return the mean of y_train\n",
    "    return np.mean(y_train)\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "\n",
    "y_train_avg = compute_average(y_train)\n",
    "print(\"Average of y on the training label values is {}\".format(y_train_avg))\n",
    "\n",
    "# The trivial predictor returns the average value.\n",
    "def trivial_predictor(X_test, y_train_avg):\n",
    "  return y_train_avg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x531Q_SxXV14"
   },
   "source": [
    "### **Task 3**\n",
    "We next evaluate the trivial predictor on the training data and test data. \n",
    "We use mean squared error (MSE) to measure the performance of the predictor.\n",
    "The task is to implement a function that reports the mean squared error of the given predictor on the given data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1394,
     "status": "ok",
     "timestamp": 1596436129240,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "mV8l6Ci9YlgL",
    "outputId": "f57858dc-d0fc-40fe-dbf7-c652d2f8fddb",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHERE AM I ??\n",
      "(3918, 11)\n",
      "[[ 0.88871893 -0.38559258  0.84593335 ... -0.84345239 -1.05366384\n",
      "   0.88824123]\n",
      " [ 0.05449268  1.32186873 -0.0393787  ...  0.09019543  0.27386315\n",
      "  -0.90714153]\n",
      " [ 3.74892322 -0.78734818  0.44351878 ... -1.24358716 -0.61115484\n",
      "  -0.09105846]\n",
      " ...\n",
      " [ 1.24624447  0.71923533 -0.76372492 ... -1.04351977  0.89337574\n",
      "  -0.00945015]\n",
      " [-1.1372591   0.61879643 -0.28082744 ... -0.643385   -1.31916923\n",
      "   1.21467446]\n",
      " [-0.66055839  0.21704082 -0.76372492 ...  0.22357369 -0.16864585\n",
      "  -0.82553323]]\n",
      "(3918,)\n",
      "[6. 5. 6. ... 6. 6. 5.]\n",
      "y_predicted:\n",
      "[5.87876468 5.87876468 5.87876468 ... 5.87876468 5.87876468 5.87876468]\n",
      "WHERE AM I ??\n",
      "(980, 11)\n",
      "[[ 6.7   0.34  0.54 ...  3.04  0.56  8.8 ]\n",
      " [ 7.9   0.33  0.28 ...  3.15  0.38  8.8 ]\n",
      " [ 7.2   0.31  0.35 ...  3.14  0.53  9.7 ]\n",
      " ...\n",
      " [ 6.3   0.48  0.04 ...  3.24  0.36  9.6 ]\n",
      " [ 5.8   0.13  0.22 ...  3.32  0.42 11.7 ]\n",
      " [ 7.8   0.32  0.33 ...  3.07  0.58  9.6 ]]\n",
      "(980,)\n",
      "[5. 6. 5. 6. 5. 5. 5. 7. 5. 6. 7. 5. 6. 7. 6. 5. 5. 5. 6. 7. 7. 6. 7. 7.\n",
      " 8. 3. 5. 6. 6. 5. 4. 7. 6. 6. 6. 7. 6. 5. 7. 5. 7. 7. 6. 5. 5. 7. 5. 7.\n",
      " 4. 6. 6. 6. 6. 8. 6. 7. 5. 6. 7. 6. 6. 6. 6. 6. 6. 6. 6. 7. 6. 6. 5. 7.\n",
      " 5. 7. 6. 7. 7. 6. 5. 6. 4. 7. 7. 6. 5. 6. 6. 6. 7. 5. 4. 7. 6. 5. 5. 6.\n",
      " 6. 6. 7. 7. 6. 7. 6. 7. 7. 6. 5. 5. 5. 5. 5. 8. 6. 6. 6. 5. 6. 6. 6. 6.\n",
      " 6. 6. 5. 5. 6. 5. 7. 6. 6. 8. 6. 6. 4. 6. 7. 6. 6. 6. 6. 6. 6. 7. 5. 6.\n",
      " 6. 8. 5. 6. 5. 6. 6. 6. 5. 6. 7. 5. 5. 5. 4. 7. 6. 7. 5. 7. 5. 6. 6. 5.\n",
      " 5. 5. 4. 7. 6. 6. 7. 6. 6. 7. 6. 6. 5. 7. 6. 8. 5. 6. 6. 5. 5. 5. 6. 5.\n",
      " 5. 7. 5. 7. 5. 6. 6. 5. 7. 8. 7. 6. 6. 7. 6. 8. 7. 7. 5. 6. 3. 5. 6. 6.\n",
      " 6. 6. 5. 6. 6. 8. 7. 6. 6. 6. 6. 8. 7. 6. 8. 6. 6. 5. 6. 6. 6. 7. 6. 5.\n",
      " 6. 6. 7. 5. 7. 6. 6. 6. 3. 6. 5. 6. 6. 5. 7. 6. 5. 6. 7. 6. 7. 6. 6. 7.\n",
      " 7. 7. 5. 5. 6. 6. 7. 5. 6. 6. 5. 7. 6. 5. 5. 7. 6. 6. 6. 5. 5. 3. 5. 6.\n",
      " 6. 6. 5. 6. 6. 7. 6. 4. 6. 5. 5. 5. 6. 6. 6. 7. 6. 7. 7. 8. 5. 3. 5. 7.\n",
      " 6. 7. 6. 7. 6. 8. 6. 7. 6. 6. 6. 7. 5. 6. 5. 7. 6. 8. 8. 5. 6. 6. 6. 6.\n",
      " 5. 6. 5. 6. 5. 5. 6. 7. 6. 5. 5. 5. 5. 5. 6. 6. 9. 6. 5. 5. 5. 6. 7. 7.\n",
      " 5. 6. 6. 7. 6. 7. 6. 6. 6. 7. 5. 6. 5. 6. 6. 5. 7. 6. 6. 6. 5. 5. 6. 5.\n",
      " 4. 6. 6. 5. 6. 5. 5. 7. 6. 6. 6. 5. 6. 5. 8. 5. 5. 6. 5. 7. 6. 5. 7. 5.\n",
      " 6. 6. 5. 5. 6. 5. 6. 5. 7. 7. 5. 5. 6. 5. 4. 7. 6. 6. 5. 5. 6. 5. 7. 5.\n",
      " 5. 6. 6. 6. 4. 6. 6. 6. 6. 5. 5. 6. 6. 6. 6. 6. 6. 5. 5. 5. 6. 6. 7. 5.\n",
      " 5. 7. 6. 7. 7. 6. 5. 6. 6. 6. 6. 6. 7. 6. 5. 7. 5. 6. 4. 5. 4. 5. 7. 4.\n",
      " 4. 7. 6. 7. 6. 6. 6. 7. 7. 6. 6. 5. 5. 6. 6. 4. 6. 5. 6. 4. 6. 6. 6. 6.\n",
      " 8. 7. 6. 6. 7. 8. 6. 7. 7. 6. 5. 7. 5. 6. 6. 6. 6. 5. 6. 6. 5. 6. 5. 6.\n",
      " 7. 7. 5. 6. 6. 5. 5. 6. 5. 6. 5. 5. 5. 4. 7. 6. 7. 6. 5. 5. 6. 5. 5. 5.\n",
      " 7. 6. 6. 6. 5. 6. 6. 6. 6. 4. 6. 7. 6. 6. 5. 7. 6. 5. 5. 6. 5. 6. 6. 6.\n",
      " 6. 5. 5. 7. 6. 5. 5. 7. 4. 7. 5. 6. 5. 6. 5. 6. 7. 7. 6. 5. 5. 5. 6. 7.\n",
      " 5. 6. 6. 7. 6. 5. 6. 6. 6. 3. 5. 6. 5. 8. 6. 6. 5. 6. 5. 6. 5. 6. 7. 5.\n",
      " 5. 6. 5. 5. 5. 6. 6. 7. 5. 7. 7. 6. 6. 6. 7. 6. 7. 7. 5. 5. 7. 7. 8. 5.\n",
      " 6. 6. 6. 6. 6. 6. 6. 5. 6. 7. 8. 7. 6. 6. 7. 5. 5. 6. 6. 7. 7. 6. 7. 7.\n",
      " 5. 6. 6. 5. 6. 5. 5. 5. 7. 9. 7. 6. 6. 6. 6. 5. 5. 7. 6. 4. 4. 4. 6. 7.\n",
      " 6. 6. 6. 6. 6. 5. 5. 4. 5. 5. 6. 5. 5. 7. 6. 7. 8. 5. 7. 5. 6. 5. 4. 6.\n",
      " 7. 7. 5. 5. 6. 5. 5. 5. 5. 5. 6. 5. 6. 6. 7. 6. 6. 6. 5. 5. 7. 6. 6. 6.\n",
      " 6. 5. 7. 4. 6. 8. 6. 6. 6. 5. 6. 6. 7. 7. 5. 5. 6. 5. 5. 4. 7. 6. 7. 6.\n",
      " 5. 6. 6. 6. 5. 6. 6. 4. 7. 5. 7. 7. 6. 6. 7. 6. 7. 6. 5. 5. 6. 7. 5. 7.\n",
      " 5. 6. 4. 6. 6. 5. 5. 5. 6. 6. 6. 6. 5. 5. 7. 5. 6. 7. 7. 7. 7. 7. 5. 6.\n",
      " 4. 6. 7. 6. 7. 6. 5. 5. 6. 6. 5. 7. 7. 5. 5. 5. 6. 5. 6. 5. 6. 5. 8. 6.\n",
      " 7. 4. 7. 6. 6. 8. 5. 5. 5. 5. 6. 5. 5. 6. 6. 6. 7. 7. 5. 6. 6. 6. 6. 6.\n",
      " 6. 5. 5. 7. 6. 5. 8. 5. 6. 8. 6. 6. 5. 7. 8. 5. 5. 5. 6. 6. 5. 5. 8. 6.\n",
      " 7. 5. 7. 6. 6. 7. 7. 6. 5. 7. 7. 4. 4. 6. 6. 6. 6. 6. 6. 5. 6. 8. 7. 6.\n",
      " 6. 6. 6. 5. 6. 6. 7. 6. 5. 6. 5. 6. 7. 8. 7. 6. 5. 5. 5. 5. 5. 5. 6. 6.\n",
      " 6. 5. 7. 5. 6. 6. 5. 5. 5. 5. 7. 7. 7. 4. 7. 5. 6. 5. 5. 5. 7. 6. 5. 5.\n",
      " 6. 7. 6. 5. 6. 5. 5. 5. 5. 5. 6. 6. 6. 5. 6. 6. 6. 6. 6. 6.]\n",
      "y_predicted:\n",
      "[5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468 5.87876468\n",
      " 5.87876468 5.87876468]\n",
      "Trivial Predictor\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "MSE (Training) = 0.7768\n",
      "MSE (Testing)  = 0.8139\n"
     ]
    }
   ],
   "source": [
    "# We next test our trivial predictor on the training data and test data. \n",
    "# Implement a function that can report the mean squared error \n",
    "# of a predictor on the given data\n",
    "# Input: data and predictor\n",
    "# Output: mean squared error of the predictor on the given data\n",
    "def test_predictor(X, y, predictor: callable=None):\n",
    "    \n",
    "    print('WHERE AM I ??')\n",
    "    print(X.shape)\n",
    "    print(X)\n",
    "    print(y.shape)\n",
    "    print(y)\n",
    "    \n",
    "    # Apply the predictor to each row of the matrix X to get the predictions\n",
    "    y_predicted = np.apply_along_axis(predictor, 1, X)\n",
    "\n",
    "    print('y_predicted:')\n",
    "    print(y_predicted)\n",
    "    \n",
    "    # TODO: compute the mean squared error of y\n",
    "    # The code below is just for compilation. \n",
    "    # You need to delete it and write your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    sum_of_squared_errors = 0\n",
    "    for i in range(len(y)):\n",
    "        # print(f'{y[i]} -> {y_predicted[i]}')\n",
    "        sum_of_squared_errors += (y[i] - y_predicted[i])**2\n",
    "    mse = sum_of_squared_errors / len(y)\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "    \n",
    "    return mse\n",
    "\n",
    "# use the function test_predictor to test the trivial predictor\n",
    "# we use the lambda function here to pass the function trivial predictor to the function test_predictor.\n",
    "mse_trivial_predictor_train = test_predictor(X_train, y_train, lambda x: trivial_predictor(x, y_train_avg))\n",
    "mse_trivial_predictor_test = test_predictor(X_test, y_test, lambda x: trivial_predictor(x, y_train_avg))\n",
    "\n",
    "# Report the result\n",
    "print('Trivial Predictor')\n",
    "print('--------------------------------------------------------------------------------\\n')\n",
    "print('MSE (Training) = %.4f' % mse_trivial_predictor_train)\n",
    "print('MSE (Testing)  = %.4f' % mse_trivial_predictor_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "geiyM1Nea0az"
   },
   "source": [
    "## Train the Linear Model Using Least Squares Method\n",
    "\n",
    "Let us train a linear model on the training data and then check its MSE. \n",
    "We use the closed form solution of the least squares estimate to get the parameters of the linear model. \n",
    "The linear model should perform better than the trivial predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WSEwFGp_bqAI"
   },
   "source": [
    "### **Task 4**\n",
    "Before training the model, we need to standardize the data, i.e., transform the data so that every feature has mean 0 and variance 1. \n",
    "\n",
    "https://en.wikipedia.org/wiki/Standard_score\n",
    "\n",
    "We first standardize the training data. \n",
    "Then we apply the same transformation to the test data, i.e. standardize the test data using the means and the standard deviations of the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1387,
     "status": "ok",
     "timestamp": 1596436129241,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "trjwkcgybhDH",
    "outputId": "d87a4635-354f-47e2-947a-e843f027e4cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_std: (3918, 11)\n",
      "Mean: [ 1.22413565e-17 -8.16090432e-18  1.45082743e-17  6.34737002e-18\n",
      "  5.44060288e-18 -8.61428789e-18  1.08812058e-17  9.06767146e-18\n",
      "  1.45082743e-17 -7.25413717e-18  2.17624115e-17]\n",
      "Standard deviation: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# The task is to implement a function that can standardize the data and returns the mean and std of the data.\n",
    "# Input: training data\n",
    "# Output: standardize training data, standard deviations and means\n",
    "def standardize_data(X):\n",
    "    # TODO: compute the means and standard deviations of the data, and standardize the data\n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    mean = np.zeros(X.shape[1])\n",
    "    std = np.zeros(X.shape[1])\n",
    "    X_std = np.zeros(X.shape)\n",
    "    \n",
    "    num_of_rows, num_of_cols = X.shape\n",
    "    \n",
    "    for col_index in range(num_of_cols):\n",
    "        col = X[:,col_index]\n",
    "        mean[col_index] = np.mean(col)\n",
    "        # std[col_index] = np.std(col, axis=0) # strange ??\n",
    "        \n",
    "        #isnt it strange that the standard deviation is 1 ??\n",
    "        \n",
    "        sum_of_squared_deviations_from_the_mean = 0\n",
    "        for row_index in range(num_of_rows):\n",
    "            sum_of_squared_deviations_from_the_mean += np.abs(X[row_index, col_index] - mean[col_index])**2\n",
    "        std[col_index] = (sum_of_squared_deviations_from_the_mean / num_of_rows)**0.5\n",
    "        \n",
    "        for row_index in range(num_of_rows):\n",
    "            z = (X[row_index, col_index] - mean[col_index]) / std[col_index]\n",
    "            X_std[row_index, col_index] = z\n",
    "    \n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "    \n",
    "    return X_std, mean, std\n",
    "\n",
    "# Standardize the training data and store the means and the stds \n",
    "X_train_std, X_train_mean, X_train_std_div = standardize_data(X_train)\n",
    "print(\"X_train_std:\", X_train_std.shape)\n",
    "print(\"Mean:\", X_train_mean)\n",
    "print(\"Standard deviation:\", X_train_std_div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1380,
     "status": "ok",
     "timestamp": 1596436129242,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "RjzbA5JpM759",
    "outputId": "ff594788-2fdd-419c-98fa-beac6a53cfc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(980, 11)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Standardize the test data using the means and standrad deviations of the training data\n",
    "###################################################\n",
    "##### YOUR CODE STARTS HERE #######################\n",
    "###################################################\n",
    "X_test_std = X_test\n",
    "num_of_rows, num_of_cols = X_test.shape\n",
    "\n",
    "for row_index in range(num_of_rows):\n",
    "    for col_index in range(num_of_cols):\n",
    "        z = (X_test[row_index, col_index] - X_train_mean[col_index]) / X_train_std_div[col_index]\n",
    "        X_test_std[row_index, col_index] = z\n",
    "\n",
    "print(X_test_std.shape)\n",
    "###################################################\n",
    "##### YOUR CODE ENDS HERE #########################\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cRPPA6HMbNOr"
   },
   "source": [
    "### **Task 5**\n",
    "We have standardized X-values. Do we need to standardize the y-values? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9he5QMmfqL3_"
   },
   "source": [
    "Answer: Standardizing the y-values is generally not needed. It can be helpful though to compare results and may improve calculation speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vT4_Sl42bxmD"
   },
   "source": [
    "### **Task 6**\n",
    "Let's now train the linear model using the least-squares method. \n",
    "We need to add the bias term to the matrix X. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1374,
     "status": "ok",
     "timestamp": 1596436129242,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "A4JtLr6pdJV7",
    "outputId": "dfd57312-284f-4ce9-820b-4fdbdfbec8c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: (12,)\n"
     ]
    }
   ],
   "source": [
    "# The task is to implement the function that adds a column of ones to the front of the input matrix\n",
    "def expand_with_ones(X):\n",
    "    # TODO: add a column of ones to the front of the input matrix\n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    column_of_ones = np.ones((X.shape[0], 1))\n",
    "    X_out = np.hstack((column_of_ones, X))\n",
    "    return X_out\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "\n",
    "# Train the linear model using the least-squares method\n",
    "# The task is to implement the function that computes the parameters\n",
    "def least_squares_compute_parameters(X_input, y):\n",
    "    # add the bias column to the data\n",
    "    X = expand_with_ones(X_input)\n",
    "\n",
    "    # TODO: compute the parameters based on the expanded X and y using the least-squares method\n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    \n",
    "    # implementation of formula w = (XTX)^-1XTy\n",
    "    X_trans = X.transpose()\n",
    "    w = np.dot(np.dot(np.linalg.inv(np.dot(X_trans, X)), X_trans), y)\n",
    "    \n",
    "    return w\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "\n",
    "# Apply the function to train the linear model\n",
    "w = least_squares_compute_parameters(X_train_std, y_train) \n",
    "print(\"w:\", w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lasj_1PpeZib"
   },
   "source": [
    "After computing the parameters,\n",
    "we can build the linear model predictor.\n",
    "The predictor takes as input the computed parameters and the data, and predicts the labels for the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lb-hNagxc3Wj"
   },
   "outputs": [],
   "source": [
    "# Implement the linear model predictor\n",
    "# Input: test data and parameters\n",
    "# Output: predicted values\n",
    "def linear_model_predictor(X, w):\n",
    "    # TODO: predict the labels for the input data\n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    \n",
    "    # print('inside linear_model_predictor')\n",
    "    # print(X)\n",
    "    # print(w)\n",
    "    \n",
    "    y_predicted = np.dot(X, w)\n",
    "    return y_predicted\n",
    "\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.08613445 6.53361345 2.1127451 ]\n",
      "(5, 3)\n",
      "WHERE AM I ??\n",
      "(5, 3)\n",
      "[[1.  2.7 1. ]\n",
      " [1.  4.1 1. ]\n",
      " [1.  1.  0. ]\n",
      " [1.  5.2 1. ]\n",
      " [1.  2.8 0. ]]\n",
      "(5,)\n",
      "[25 33 15 45 22]\n",
      "y_predicted:\n",
      "[25.83963585 34.98669468 12.6197479  42.17366947 24.3802521 ]\n",
      "mse: 4.794257703081233\n",
      "[25.83963585 34.98669468 12.6197479  42.17366947 24.3802521 ]\n"
     ]
    }
   ],
   "source": [
    "# this code is only for me to test if it is working with the demo data from the lecture script. (it does..)\n",
    "\n",
    "X_demo = np.array([\n",
    "    [2.7, 1],\n",
    "    [4.1, 1],\n",
    "    [1.0, 0],\n",
    "    [5.2, 1],\n",
    "    [2.8, 0]\n",
    "])\n",
    "y_demo = np.array([25, 33, 15, 45, 22])\n",
    "\n",
    "w_demo = least_squares_compute_parameters(X, y) \n",
    "print(w_demo)\n",
    "\n",
    "print(expand_with_ones(X_demo).shape)\n",
    "# y_predicted_demo = linear_model_predictor(expand_with_ones(X_demo), w_demo)\n",
    "mse_linear_model_predictor_demo = test_predictor(expand_with_ones(X_demo), y_demo, lambda x: linear_model_predictor(x, w_demo))\n",
    "print('mse:', mse_linear_model_predictor_demo)\n",
    "\n",
    "print(y_predicted_demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cFOYpwbufz7J"
   },
   "source": [
    "We can now evaluate our linear model predictor on the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1363,
     "status": "ok",
     "timestamp": 1596436129243,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "LuHHmn2RB55j",
    "outputId": "b6cb4556-2618-419a-a082-214f2e6ecb5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHERE AM I ??\n",
      "(980, 12)\n",
      "[[ 1.    6.7   0.34 ...  3.04  0.56  8.8 ]\n",
      " [ 1.    7.9   0.33 ...  3.15  0.38  8.8 ]\n",
      " [ 1.    7.2   0.31 ...  3.14  0.53  9.7 ]\n",
      " ...\n",
      " [ 1.    6.3   0.48 ...  3.24  0.36  9.6 ]\n",
      " [ 1.    5.8   0.13 ...  3.32  0.42 11.7 ]\n",
      " [ 1.    7.8   0.32 ...  3.07  0.58  9.6 ]]\n",
      "(980,)\n",
      "[5. 6. 5. 6. 5. 5. 5. 7. 5. 6. 7. 5. 6. 7. 6. 5. 5. 5. 6. 7. 7. 6. 7. 7.\n",
      " 8. 3. 5. 6. 6. 5. 4. 7. 6. 6. 6. 7. 6. 5. 7. 5. 7. 7. 6. 5. 5. 7. 5. 7.\n",
      " 4. 6. 6. 6. 6. 8. 6. 7. 5. 6. 7. 6. 6. 6. 6. 6. 6. 6. 6. 7. 6. 6. 5. 7.\n",
      " 5. 7. 6. 7. 7. 6. 5. 6. 4. 7. 7. 6. 5. 6. 6. 6. 7. 5. 4. 7. 6. 5. 5. 6.\n",
      " 6. 6. 7. 7. 6. 7. 6. 7. 7. 6. 5. 5. 5. 5. 5. 8. 6. 6. 6. 5. 6. 6. 6. 6.\n",
      " 6. 6. 5. 5. 6. 5. 7. 6. 6. 8. 6. 6. 4. 6. 7. 6. 6. 6. 6. 6. 6. 7. 5. 6.\n",
      " 6. 8. 5. 6. 5. 6. 6. 6. 5. 6. 7. 5. 5. 5. 4. 7. 6. 7. 5. 7. 5. 6. 6. 5.\n",
      " 5. 5. 4. 7. 6. 6. 7. 6. 6. 7. 6. 6. 5. 7. 6. 8. 5. 6. 6. 5. 5. 5. 6. 5.\n",
      " 5. 7. 5. 7. 5. 6. 6. 5. 7. 8. 7. 6. 6. 7. 6. 8. 7. 7. 5. 6. 3. 5. 6. 6.\n",
      " 6. 6. 5. 6. 6. 8. 7. 6. 6. 6. 6. 8. 7. 6. 8. 6. 6. 5. 6. 6. 6. 7. 6. 5.\n",
      " 6. 6. 7. 5. 7. 6. 6. 6. 3. 6. 5. 6. 6. 5. 7. 6. 5. 6. 7. 6. 7. 6. 6. 7.\n",
      " 7. 7. 5. 5. 6. 6. 7. 5. 6. 6. 5. 7. 6. 5. 5. 7. 6. 6. 6. 5. 5. 3. 5. 6.\n",
      " 6. 6. 5. 6. 6. 7. 6. 4. 6. 5. 5. 5. 6. 6. 6. 7. 6. 7. 7. 8. 5. 3. 5. 7.\n",
      " 6. 7. 6. 7. 6. 8. 6. 7. 6. 6. 6. 7. 5. 6. 5. 7. 6. 8. 8. 5. 6. 6. 6. 6.\n",
      " 5. 6. 5. 6. 5. 5. 6. 7. 6. 5. 5. 5. 5. 5. 6. 6. 9. 6. 5. 5. 5. 6. 7. 7.\n",
      " 5. 6. 6. 7. 6. 7. 6. 6. 6. 7. 5. 6. 5. 6. 6. 5. 7. 6. 6. 6. 5. 5. 6. 5.\n",
      " 4. 6. 6. 5. 6. 5. 5. 7. 6. 6. 6. 5. 6. 5. 8. 5. 5. 6. 5. 7. 6. 5. 7. 5.\n",
      " 6. 6. 5. 5. 6. 5. 6. 5. 7. 7. 5. 5. 6. 5. 4. 7. 6. 6. 5. 5. 6. 5. 7. 5.\n",
      " 5. 6. 6. 6. 4. 6. 6. 6. 6. 5. 5. 6. 6. 6. 6. 6. 6. 5. 5. 5. 6. 6. 7. 5.\n",
      " 5. 7. 6. 7. 7. 6. 5. 6. 6. 6. 6. 6. 7. 6. 5. 7. 5. 6. 4. 5. 4. 5. 7. 4.\n",
      " 4. 7. 6. 7. 6. 6. 6. 7. 7. 6. 6. 5. 5. 6. 6. 4. 6. 5. 6. 4. 6. 6. 6. 6.\n",
      " 8. 7. 6. 6. 7. 8. 6. 7. 7. 6. 5. 7. 5. 6. 6. 6. 6. 5. 6. 6. 5. 6. 5. 6.\n",
      " 7. 7. 5. 6. 6. 5. 5. 6. 5. 6. 5. 5. 5. 4. 7. 6. 7. 6. 5. 5. 6. 5. 5. 5.\n",
      " 7. 6. 6. 6. 5. 6. 6. 6. 6. 4. 6. 7. 6. 6. 5. 7. 6. 5. 5. 6. 5. 6. 6. 6.\n",
      " 6. 5. 5. 7. 6. 5. 5. 7. 4. 7. 5. 6. 5. 6. 5. 6. 7. 7. 6. 5. 5. 5. 6. 7.\n",
      " 5. 6. 6. 7. 6. 5. 6. 6. 6. 3. 5. 6. 5. 8. 6. 6. 5. 6. 5. 6. 5. 6. 7. 5.\n",
      " 5. 6. 5. 5. 5. 6. 6. 7. 5. 7. 7. 6. 6. 6. 7. 6. 7. 7. 5. 5. 7. 7. 8. 5.\n",
      " 6. 6. 6. 6. 6. 6. 6. 5. 6. 7. 8. 7. 6. 6. 7. 5. 5. 6. 6. 7. 7. 6. 7. 7.\n",
      " 5. 6. 6. 5. 6. 5. 5. 5. 7. 9. 7. 6. 6. 6. 6. 5. 5. 7. 6. 4. 4. 4. 6. 7.\n",
      " 6. 6. 6. 6. 6. 5. 5. 4. 5. 5. 6. 5. 5. 7. 6. 7. 8. 5. 7. 5. 6. 5. 4. 6.\n",
      " 7. 7. 5. 5. 6. 5. 5. 5. 5. 5. 6. 5. 6. 6. 7. 6. 6. 6. 5. 5. 7. 6. 6. 6.\n",
      " 6. 5. 7. 4. 6. 8. 6. 6. 6. 5. 6. 6. 7. 7. 5. 5. 6. 5. 5. 4. 7. 6. 7. 6.\n",
      " 5. 6. 6. 6. 5. 6. 6. 4. 7. 5. 7. 7. 6. 6. 7. 6. 7. 6. 5. 5. 6. 7. 5. 7.\n",
      " 5. 6. 4. 6. 6. 5. 5. 5. 6. 6. 6. 6. 5. 5. 7. 5. 6. 7. 7. 7. 7. 7. 5. 6.\n",
      " 4. 6. 7. 6. 7. 6. 5. 5. 6. 6. 5. 7. 7. 5. 5. 5. 6. 5. 6. 5. 6. 5. 8. 6.\n",
      " 7. 4. 7. 6. 6. 8. 5. 5. 5. 5. 6. 5. 5. 6. 6. 6. 7. 7. 5. 6. 6. 6. 6. 6.\n",
      " 6. 5. 5. 7. 6. 5. 8. 5. 6. 8. 6. 6. 5. 7. 8. 5. 5. 5. 6. 6. 5. 5. 8. 6.\n",
      " 7. 5. 7. 6. 6. 7. 7. 6. 5. 7. 7. 4. 4. 6. 6. 6. 6. 6. 6. 5. 6. 8. 7. 6.\n",
      " 6. 6. 6. 5. 6. 6. 7. 6. 5. 6. 5. 6. 7. 8. 7. 6. 5. 5. 5. 5. 5. 5. 6. 6.\n",
      " 6. 5. 7. 5. 6. 6. 5. 5. 5. 5. 7. 7. 7. 4. 7. 5. 6. 5. 5. 5. 7. 6. 5. 5.\n",
      " 6. 7. 6. 5. 6. 5. 5. 5. 5. 5. 6. 6. 6. 5. 6. 6. 6. 6. 6. 6.]\n",
      "y_predicted:\n",
      "[16.86200278 22.6706984  13.47156315 13.98576302  9.6963706  11.09494671\n",
      " 11.71348263 11.44899991  9.76826571 10.40700957 11.91802342 10.6092054\n",
      " 11.93411071 17.48341927 17.74592722 10.15789006 16.10986933 14.17351896\n",
      " 12.58337531 15.29504618 14.37944924 11.5388372  12.7835619  11.07964048\n",
      " 11.61308985  9.38170744 13.49419672 12.08474542 14.28933037 11.3190725\n",
      "  9.92444616 11.29314141 11.6446294  12.73185451 16.1408687   9.53596828\n",
      " 12.61339562 16.49002875 13.21125515 16.20607841 13.8114238  12.35354595\n",
      " 17.95833449  9.82829811 12.77477336 12.275657   12.9646304  16.78348416\n",
      " 10.8355687  12.96212829 17.09554264 11.21290277 10.97304023 11.69570208\n",
      " 10.96560831 13.41917056 11.36491572 17.98767564 14.31125385 15.66867709\n",
      " 10.50361615 10.6631791  10.55940608 12.67423436 16.01613088 11.12997035\n",
      " 10.35443576 10.80416154 10.79681397 15.59809947 13.64649549 16.36276799\n",
      " 13.90117874 11.72830949 10.36701442 15.02068382 13.5615769  12.43014041\n",
      " 14.53553633 13.83060034 13.51561245 11.21533896 12.64387464 10.99828277\n",
      " 14.90585448 11.10245362 12.52607568 10.82296819 11.00907859  9.27866393\n",
      " 10.06893529 11.4787471  14.04347135 14.97328189 14.28338971 10.67306797\n",
      " 13.90350592 12.26436466 11.64439359 10.77635649 10.58739564 11.44899991\n",
      "  9.70500561 11.19211111 11.52617947 10.68015687 11.42243539 10.33588369\n",
      " 10.00760287 16.63079085 12.13519289 12.01104341 15.9015009   9.93137313\n",
      " 14.80245254  9.25737048 14.0661228  10.49893828 11.91401753 10.95880294\n",
      " 12.83960476 12.09752823 12.36870432  9.40815314 13.23682896 16.87859718\n",
      " 11.55827187 11.21223879 11.45156208 10.80103655 12.57025279 11.38107984\n",
      " 10.38963171 16.25490578  9.96658065 16.31973861 11.2703376  13.57591643\n",
      "  9.84090801 14.11626129 12.64474521 11.07200398 13.43053218 14.62577206\n",
      " 15.5796272  13.72835018 16.0084306  12.10880025 12.78458357 10.9206315\n",
      " 13.67373045 12.97331962 15.88670804 11.79657639 11.21603854 11.93432952\n",
      " 11.88308145 11.01576817 14.74952803 14.10314112 10.75579858 15.02068382\n",
      " 17.26002556 11.6888972  16.47503559 15.09298313 14.76512715 16.29144987\n",
      " 10.74754795  9.71560487 11.96449178 11.38437761 13.09647926 16.26403098\n",
      " 16.6421232  10.78738348 12.42564147 11.38423757 10.87760323 14.26943024\n",
      " 17.35700917 11.88782026 11.54367741 11.05342049 13.46378815 12.07276918\n",
      " 10.46622297 10.61708528  9.5712999  14.06346798 12.34486515 12.58982718\n",
      " 15.98432143 13.10482984 14.17895042 12.19479214 11.03316879 12.71626707\n",
      " 11.32846824 11.11475783 11.9975454  12.45858974 10.82000008 13.30776225\n",
      " 10.34726634 10.96875943 11.82791535 10.80103655 10.66157562 11.52234697\n",
      " 14.3926057  12.60236077 13.79391347 12.7945627  10.47403179 13.71584962\n",
      " 10.52397195 13.00154529 13.90985411 11.64394031 15.06204824 11.69395183\n",
      " 12.80475129 16.08136467 12.73704457 10.66042943 12.52082463 12.02311269\n",
      " 10.80416154 14.0106223  12.62786552 15.85574485 17.34544183 15.28968715\n",
      " 10.69649631 15.82929979 14.06000159 15.22060454 12.65012537 10.32434065\n",
      " 10.21759104 11.67251834 13.69049102 11.19771229 14.65781311 11.63753191\n",
      " 11.1927929  14.72503595 17.1784748  14.11896724 11.33925713 10.43692313\n",
      " 13.39580132  9.95946585 14.62706644 14.60709654  9.057608   10.43391912\n",
      " 12.25612557 12.93534654 14.09298297 14.93930018 13.0054797  13.93934571\n",
      " 10.94452818 13.74329818 16.60393086 11.53834027 10.23492563 13.8055041\n",
      " 13.57553574 15.72095896 16.99940449 11.11711215 11.61823623 13.66155779\n",
      " 10.27872049 10.36859814 15.86508262 13.9842862  16.99940449 10.46364271\n",
      " 11.47061861 10.77654005 10.02316975 10.23906587 13.44448563  8.9043231\n",
      " 10.59685648 14.64077566 10.71279738 11.8550145  11.37790508 11.74459267\n",
      " 11.15566123  9.71743149 11.19442935 10.5421148  11.65505917 10.37451567\n",
      " 12.17116284 10.40816766 10.4988159  11.90870462 16.26825125 11.90024912\n",
      " 12.03557656 10.19427094 14.99014975 19.0016004  12.85671956 12.43955586\n",
      " 13.93424075 10.25853511 10.52730554 12.91547805 13.04169331 13.92784483\n",
      " 10.79259852 12.58541048 11.74029709 15.76975215 11.75164963 11.09734954\n",
      " 16.67926529 17.44117599 13.45815877 15.67224323 10.58507361 11.05399321\n",
      " 16.66107466 14.24842052 13.56311602 15.82929979 14.03651494 12.78635755\n",
      " 15.97453621  9.99692499 10.77063644 17.13273045 15.580263   12.89938044\n",
      " 17.09554264 13.9842862  15.59809947 13.96989764 14.08258506 12.70859762\n",
      " 12.9646304  15.78412519 11.66839196 10.37079764 13.87288345 10.22217317\n",
      " 16.14749819 12.72428602 13.46378815 10.48614613 12.38453464 11.74155785\n",
      "  9.54433154 10.33559046 11.55353982 13.41350948 15.38814686 12.97090312\n",
      "  9.6611983  15.4275935  13.3304424  13.03633207 16.51952911 13.14809774\n",
      " 15.19149376 13.91552041 10.23122088 14.2085138  11.00367674 17.17714786\n",
      " 11.20674293 11.94833435 16.72994605 16.11337037 12.65603424 10.04124248\n",
      " 14.40036078 13.77061301 11.47061861 12.65333968 10.55925998 10.11470902\n",
      "  9.59427658 14.05979015 10.10536792 15.58065045  9.57610008  9.18036144\n",
      " 16.28186147 14.85301251 11.41866816 13.17811615 11.66601487 13.26195236\n",
      " 12.99804228 15.59977655 10.67769655 12.33087811 11.10472275 18.0530338\n",
      " 10.146328   17.56369495 14.17343148 10.41981202 10.82296819 15.25739622\n",
      " 13.60071102 18.28381188 11.70903853  9.78663636 14.75015374 14.72980705\n",
      " 11.19442935 11.90444517 12.58885638 14.043612   12.17430011 10.08910963\n",
      " 15.38962113 14.30709321 14.24636376 10.76219986 10.56156288 10.62810104\n",
      " 14.90312186 10.79463117 11.6867956  10.39318067 16.09365261 12.6791499\n",
      " 12.29064947 10.13798238 17.72858293 13.40286465 14.15195783 13.26632953\n",
      " 10.26055605 12.51805801 12.24015122 11.21223879 10.86783693 16.89528232\n",
      "  9.82724685 16.48334675 19.77529941 11.53539561 10.4773481   9.23420448\n",
      " 16.05306797 14.0616013  12.32861862 15.59977655 10.8950819  12.06127704\n",
      " 10.13101286 14.78250353 13.43005208 11.22570091  9.53625379 17.4518166\n",
      " 12.58738385 12.31424512 11.48685252 12.59744227 14.49885301 10.90288093\n",
      "  9.08668617 10.87344936 10.55414291 13.34013671 11.77493063  9.12609305\n",
      " 10.06845462 10.75512224 13.29189333 13.72815148 10.59748888 12.06923087\n",
      " 11.26741608 15.03212736 11.03244325 13.41137845 10.81148952 14.31685793\n",
      " 15.26119069  9.49443667 11.72341571  9.34707703 11.07813494 15.02543798\n",
      " 11.02545332 13.90780486 10.66789958 13.79178998 12.74103295 10.95762713\n",
      " 10.4175753  12.14796333 13.02797364 11.66548534 12.58056948 13.5665818\n",
      " 11.53880657 11.53948113 10.7201992  16.21268479 15.29535796 11.88674218\n",
      " 14.5618599  12.58633574 18.77910395 10.35238608 10.52055585 13.92340538\n",
      " 16.15685852 11.02382368 16.06197362 11.83673177 15.71525007 10.52706752\n",
      " 12.83669284 12.17093833  9.96828463 10.07068681 13.21323728 10.42008651\n",
      " 17.22614568 11.13535074 16.4455331  11.01865575 10.32771627  9.40498169\n",
      "  9.39583212 10.42246371 11.42089889 11.51412714 15.5149818  10.86502133\n",
      " 10.35091123 10.59582457 16.43476717 10.64790552 16.72994605 17.66624409\n",
      " 10.97843539 15.94637867 13.54728662  9.17253888 18.09274411 12.0840192\n",
      " 10.22541175 10.68839261 13.7808913  10.63164937 12.51805801 11.83406331\n",
      " 11.95061981 11.47270655 10.31813114  9.58265843 10.02738182 17.71820929\n",
      " 12.77036931 15.17175814 18.29812192 13.873464   13.18376087 15.66867709\n",
      " 16.97195585 13.70181356 16.86967935 12.0970449  10.08730891 16.19647355\n",
      " 11.11993733 12.51153547  9.38693814 14.72581777  9.17607784 14.15043553\n",
      " 15.61881483 17.60186967 10.61007187 11.10520744 11.93881353 13.59900463\n",
      " 11.19606848 12.48027267 18.23461447 15.2945803  10.85563412 14.25366843\n",
      "  8.96979155 16.08453848 10.83201191 11.7169927  16.70392826 15.4866759\n",
      " 18.51904279 12.09278978 10.76257567 15.9976306  15.68605658 15.82929979\n",
      " 12.82112853 16.0755274  10.83048873  9.42244629 14.21956009 16.18252385\n",
      " 11.71348263 12.7804461   8.90690652  9.26444381 16.67487394 11.7178545\n",
      " 13.03937013 10.9905348  14.13771105 13.7715855  10.06323883 13.4517507\n",
      " 14.13472769 11.79026937 14.94340736 15.5149818  10.60199114 19.57008924\n",
      " 14.45585009 10.02742398 12.19488197 13.03135272 11.65929195 10.19148213\n",
      " 10.90407359 13.49176408 13.52667132 12.40098486 16.52955736 11.39253943\n",
      " 11.14299531 11.08235361 11.93411071 14.70871429 16.01696735 12.09159494\n",
      " 15.26609103 18.26461606 11.62304059 10.3487392  16.0755274  12.85562286\n",
      " 13.38690401 16.74021141 15.63447518 15.67528667 17.28754301 11.29676945\n",
      " 10.52175839 10.50533909 11.25958319 10.32318994 15.00140412 10.42764539\n",
      " 15.91835073 11.09819084 15.28616641 10.10708627 12.76126552  9.84304739\n",
      " 10.85479509 15.89108855 12.6496942  14.33292904 12.30089989 16.84374391\n",
      " 11.96131294 11.21925524 17.09554264  9.6963706   9.60674566  9.72710431\n",
      " 13.56802559  9.95242444 15.95941727 10.44814076 11.22130011 11.27158967\n",
      " 14.13076261 10.61295009 16.70392826 10.54113042 17.2285913  15.53204037\n",
      "  9.39583212 12.42275384 15.70631938 15.56174783 13.40256264 14.79345313\n",
      " 11.13837924 14.80652678 15.18401677 17.23711903 14.13000522 11.61823623\n",
      " 11.14341923 16.15928455 10.04763641 14.25629774  9.6693772  12.7835008\n",
      " 11.14830729 12.68113922 15.4501057  12.59382119 17.9069006  16.77729482\n",
      " 14.87551569 18.15580613  9.62981371 13.71217372 13.60513041 17.78724751\n",
      " 11.59199962 13.78178114 12.13532103 12.12814492 16.15685852 15.85695756\n",
      "  9.13448059  9.94692838 10.80154014 16.45627606 11.99115559  9.96298421\n",
      "  9.56834309  9.20881159 12.80985103  9.43847599 11.33837686 13.29532763\n",
      " 18.7196635  11.60400682 11.95279379  9.76793066 10.91226572 12.70103607\n",
      " 11.15315568 10.64525916 16.23226394 11.24904446 15.12450718 14.07917361\n",
      " 13.57111226 15.97650662 17.86179488  9.63948092 10.1695565  14.37217876\n",
      " 11.73069721  9.64474076 16.01146523 13.50301632 12.84609711 16.93512815\n",
      " 11.48002623 10.20693348 13.97504423 10.1589861  10.41071943 16.3406863\n",
      "  9.66250517 10.82034539 14.42890696 13.78844394 10.48277829 11.17744106\n",
      "  9.93502341 14.19559077 14.80060576 11.86044477 12.8112989   9.7691836\n",
      " 14.00305933 13.29270198  9.24752621 12.2236527  15.51134238 16.16590426\n",
      " 16.37457731 16.3586026  17.50682866 16.99130471 14.80060576 16.64812899\n",
      "  9.63486912 12.48151203 11.90682377 17.68695338 11.27391613 16.74203836\n",
      " 10.81580224 10.75120681 12.51959432 11.91802342 11.44972625 11.5589598\n",
      " 10.03114158 11.38495636 11.52617947 12.92821766 11.36318684 13.39842301\n",
      " 11.07008295 14.24555228 11.86297362 10.04020466 13.96723819 13.12541611\n",
      " 12.41857305 10.7756716  11.33486962 14.83916563 10.70242647  9.25737048\n",
      " 10.72565539 11.1476459  11.09819084 15.72738206 11.80336149 15.79751737\n",
      " 10.61033175 12.9284964  16.36276799 11.72573872 17.29135487 12.9031183\n",
      " 10.31173852 17.68695338 10.81475743 17.22356548 12.25031687 17.47485377\n",
      "  9.50129865 11.82607026 10.55940608 12.6484997  13.56878543 12.04195123\n",
      " 12.64647028 12.33741187 16.47860986 11.64850212 12.56808324 11.17734566\n",
      " 10.42032273  9.96812477 18.21823247 10.14652901 12.22000798 14.64222278\n",
      " 15.91569789 11.47209273 13.44538865 12.25044401 14.37689169 14.1088649\n",
      " 12.20395052 14.0616013  12.48085178 17.10524087 12.36173088 11.24150003\n",
      " 13.21575627 12.95164366 10.27939683  9.90204223 12.74792086 11.09693568\n",
      " 15.5149818  13.92368746 12.70713314 18.00113514 12.42930185 11.66772983\n",
      " 11.18258379 11.35564051 15.3360679  10.91342145 10.45322722  9.25238988\n",
      " 14.1686771  14.63837298 16.40813383 14.36444162 16.05148928 11.42284432\n",
      " 11.99635123 10.76941914 12.22101996 13.85577367 10.85151639 11.1890972\n",
      "  9.42244629 16.23592176  9.85745669  9.64395315 17.44319029 14.96438518\n",
      " 11.62597894 13.54374879 12.90486071 13.17508944 14.47789496 12.03192715\n",
      " 11.38480216 11.91053447 11.50290695 10.02931649 16.0084306   9.91640282\n",
      " 15.77112683 15.65661495 11.72339191 10.15723338 11.2690989  12.17005923\n",
      " 17.25363507 17.34106854 10.51323304 12.47135552 11.51483546 10.91120075\n",
      " 18.50129382 16.62208157 11.08306518 10.44662274 11.97063065 12.14165216\n",
      " 17.76290767  9.67622115 12.83669284 12.21655217 15.21243028 17.53595745\n",
      " 14.61112114 10.78173833 14.05083178 11.53678579  9.74776297 14.90780673\n",
      " 10.93352158 17.0947268  11.99312236 12.42807395 11.7265364  14.11587666\n",
      " 10.44852938 11.20216439 15.61099678 13.73845526 16.74021141 10.11538201\n",
      " 10.52706752 10.603453   10.62477281 11.98308177 13.99266387 10.26285792\n",
      " 14.90773316 14.84627547]\n",
      "Mean squared error is 55.08716167805617\n"
     ]
    }
   ],
   "source": [
    "# use the function test_predictor to evaluate the linear model predictor\n",
    "mse_linear_model_predictor = test_predictor(expand_with_ones(X_test_std), y_test, lambda x: linear_model_predictor(x, w))\n",
    "print(\"Mean squared error is {}\".format(mse_linear_model_predictor))\n",
    "\n",
    "# TODO: Something seems to be wrong.. the predicted values are way to high resulting in a MSE of 55 !?\n",
    "# it is working just fine with the commute time dummy example.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zqj4HKAihF7Q"
   },
   "source": [
    "## Learning Curves\n",
    "\n",
    "Let us check if the linear model is overfitting or underfitting. Since the dataset is somewhat large and there are only 11 features, the model shouldn't be overfitting. \n",
    "To check it, we need to check the learning curves, i.e. how the performance of the model changes when it is trained with increasingly more data. \n",
    "We train the model on the increasingly more data ([20, 40, ..., 600] data records), and evaluate the model by computing the MSE of the model on both the training data and the test data. \n",
    "We use the collected MSE to build the learning curves plot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MNf11kurCgKF"
   },
   "source": [
    "### **Task 7** \n",
    "\n",
    "Let's first implement a function that comprises what we have implemented above. \n",
    "The function takes as inputs the data and the split coefficient, and\n",
    "1. standardizes the data,\n",
    "2. trains the linear model, and\n",
    "3. reports the mse of the linear model predictor on both training and test datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1355,
     "status": "ok",
     "timestamp": 1596436129244,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "UcGRQBrEb106",
    "outputId": "179c5ec0-ee87-4c4b-a02b-d97d55862e1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Model\n",
      "-----------------------\n",
      "\n",
      "MSE (Training) = 0.0000\n",
      "MSE (Testing)  = 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Input: training data and test data\n",
    "# Output: mse of the linear model predictor on both the training and test data\n",
    "def train_and_test(X_train, y_train, X_test, y_test):\n",
    "    # TODO: implement the function \n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    # Hints: use the functions you have implemented\n",
    "    mse_train = 0\n",
    "    mse_test = 0\n",
    "    return mse_train, mse_test\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "\n",
    "mse_train, mse_test = train_and_test(X_train, y_train, X_test, y_test)\n",
    "print('Linear Model')\n",
    "print('-----------------------\\n')\n",
    "print('MSE (Training) = %.4f' % mse_train)\n",
    "print('MSE (Testing)  = %.4f' % mse_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NTJw_BrzhRwi"
   },
   "source": [
    "We are now ready to report the learning curves.\n",
    "The task is to train the linear model on the increasingly more data ([20, 40, ..., 600] data records)\n",
    "and store the MSE of the trained model on the training data and the test data in the lists `mse_train_v` and `mse_test`, respectively. \n",
    "We have provided the code for generating the learning curves from `mse_train_v` and `mse_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1747,
     "status": "ok",
     "timestamp": 1596436129644,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "jDsdh4T3hcIU",
    "outputId": "621c4890-1c55-4e9b-f28f-33d60907d8b9"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mse_train_v = []\n",
    "mse_test_v = []\n",
    "\n",
    "TRAINING_SIZE_MAX = 601\n",
    "TRAINING_SIZE_MIN = 20\n",
    "\n",
    "# compute the MSE over data with sizes from TRAINING_SIZE_MIN to TRAINING_SIZE_MAX with increasing step 20\n",
    "for train_size in range(TRAINING_SIZE_MIN, TRAINING_SIZE_MAX, 20):\n",
    "    # TODO: \n",
    "    #   1. use the first train_size data records from the X_train and y_train as the training data\n",
    "    #   2. train and compute the MSE on both training and test data using the train_and_test function\n",
    "    #   3. add the computed MSE to the lists mse_train_v and mse_test_v\n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    mse_train = 0\n",
    "    mse_test = 0\n",
    "    \n",
    "    mse_train_v.append(mse_train)\n",
    "    mse_test_v.append(mse_test)\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "    \n",
    "\n",
    "# The below code generates the learning curves plot\n",
    "plt.figure(2)\n",
    "plt.plot(np.arange(TRAINING_SIZE_MIN, TRAINING_SIZE_MAX, 20), mse_train_v, 'r--', label=\"Training Error\")\n",
    "plt.plot(np.arange(TRAINING_SIZE_MIN, TRAINING_SIZE_MAX, 20), mse_test_v, 'b-', label=\"Test Error\")\n",
    "plt.xlabel('Dataset Size')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 8**\n",
    "Explain whether you think the model is underfitting or not and how much data you need before getting the optimal test error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9A9VqDTzOdfd"
   },
   "source": [
    "(Add the answer to Task 8 here in text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "djpsaTu_kK3T"
   },
   "source": [
    "## Polynomial Basis Expansion and Regularisation\n",
    "\n",
    "In this part, we will try to improve the linear model by basis expansion and regularisation. \n",
    "\n",
    "The task is to \n",
    "1. apply the degree 2 basis expansion to the data, \n",
    "2. build the Ridge and Lasso models using scikit-learn, and\n",
    "3. perform hyperparameter optimization to find the optimal hyperparameter lambda. \n",
    "\n",
    "For the hyperparameter optimization, you should set the last 20% of the training data for the purpose of validation and \n",
    "try lambda values [10^-4, 10^-3, 10^-2, 10^-1, 1, 10, 100]. \n",
    "\n",
    "\n",
    "We will use the scikit-learn package. You can import other scikit-learn packages if you think they are useful. Read the documentation available here: http://scikit-learn.org/stable/modules/classes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9TM0nkNbkhfM"
   },
   "outputs": [],
   "source": [
    "# import the preprocessing libs for standarization and basis expansion\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures \n",
    "\n",
    "# Ridge and Lasso linear model\n",
    "from sklearn.linear_model import Ridge, Lasso "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pCwBPuOXlRF7"
   },
   "source": [
    "### **Task 9**\n",
    "Let's implement the function for expanding the basis of the dataset. \n",
    "\n",
    "Hints: use `PolynomialFeatures`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "50azFolql1qA"
   },
   "outputs": [],
   "source": [
    "def expand_basis(X, degree):\n",
    "    # TODO: expand the basis of X for the input degree\n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    # Hints: use the function PolynomialFeatures\n",
    "    return X\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6jwkPevimQri"
   },
   "source": [
    "### **Task 10**\n",
    "We need to expand and standardize the the data,\n",
    "and prepare the training, test and validation data on the expanded data. \n",
    "You should set the last 20% of the training data as the validation data.\n",
    "\n",
    "Hints: use `StandardScaler` and `std_scaler` to standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dQCq4G9YmW7w"
   },
   "outputs": [],
   "source": [
    "def prepare_data(X, y, degree):\n",
    "    # TODO: the training, test and validation data using the expanded dataset.\n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    # Hints: follow the steps     \n",
    "    # 1. split the data (X, y) into training data (X_train, y_train) and test data (X_test, y_test)\n",
    "    # 2. standardize the training data and do the same transformation to the test data\n",
    "    # 3. expand the basis of the training data and test data\n",
    "    # 4. split the expanded training data into training data (X_train_n, y_train_n) and validation data (X_train_v, y_train_v)\n",
    "    # 5. standardize the training data and do the same transformation to the validation data\n",
    "    \n",
    "    \n",
    "    # training data\n",
    "    X_train = X\n",
    "    y_train = y\n",
    "\n",
    "    # test data\n",
    "    X_test = X \n",
    "    y_test = y\n",
    "\n",
    "\n",
    "    # further split the training data to training and validation data\n",
    "    # training data\n",
    "    X_train_n = X \n",
    "    y_train_n = y\n",
    "\n",
    "    # validation data\n",
    "    X_train_v = X \n",
    "    y_train_v = y\n",
    "\n",
    "    return X_train, y_train, X_train_n, y_train_n, X_train_v, y_train_v, X_test, y_test\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "\n",
    "X_train, y_train, X_train_n, y_train_n, X_train_v, y_train_v, X_test, y_test = prepare_data(X, y, 2) # here we expand the dataset with degree 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why standardisation is required before basis expansion?**\n",
    "\n",
    "Assume we have a dataset with two features x1 and x2, where x1 has a small scale while x2 has a large scale. When we perform basis expansion, we  get a new feature x1x2. Since x2 has a larger scale than x1, it is likely x2 will contribute more to the value of the new feature x1x2, which means a bias is introduce here. \n",
    "The correct way is to standardise the features before the basis expansion. In this case x1 and x2 have the same scale, so they contribute same to the new feature x1x2, i.e. no bias is introduced. \n",
    "\n",
    "\n",
    "**Why standardise the training data in step 5?**\n",
    "\n",
    "Ridge and Lasso regularisation require the data to have mean of 0 and standard deviation of 1. However, after the basis expansion and splitting in step 4, the training data might not have the desired distribution, so we need to perform the standardisation on the training data. \n",
    "\n",
    " \n",
    "**Why not standardise both training and validation data together?**\n",
    "\n",
    "When we use validation data to chose the hyperparameters, we treat the validation data like the test data -- we should not assume we can access these data. So we should standardise the training data and perform the same operation to the validation data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i3BxxtM3nghU"
   },
   "source": [
    "### **Task 11**\n",
    "We have prepared the training data and the validation data. We can now choose the hyperparameter lambda for Ridge and Lasso using the validation data. \n",
    "We use the Ridge and Lasso models from scikit-learn: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n",
    "\n",
    "We train Ridge or Lasso models with different lambda values and check their performance on the validation data.\n",
    "The lambda value that results the best performance is then the optimal lambda. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3266,
     "status": "ok",
     "timestamp": 1596436131187,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "SvXcAGW1oHq1",
    "outputId": "25a38d1f-013f-4b0a-9cbb-3f08b68c0371"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge lambda: 0.01\n",
      "Lasso lambda: 0.01\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The function takes the training and validation data as inputs, and \n",
    "# returns the lambda value that results the minimal mse\n",
    "# We use is_ridge to indicate which the model is considered.\n",
    "# is_ridge = True indicates Ridge while is_ridge = False indicates Lasso\n",
    "def choose_hyper_param(X_train_n, y_train_n, X_train_v, y_train_v, is_ridge: bool):\n",
    "    mse_arr = []\n",
    "    lam_arr = []\n",
    "\n",
    "    # Try lambda values from 10^-4 to 10^2. \n",
    "    # Record the mse and the lambda values in mse_arr and lam_arr\n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    for pow_lam in range(-4, 2):\n",
    "        lam = 10 ** pow_lam\n",
    "        mse = 0 # compute the mse for this lam\n",
    "        mse_arr.append(mse) \n",
    "        lam_arr.append(lam)\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "\n",
    "\n",
    "    # get the index of the lambda value that has the minimal use\n",
    "    lambda_idx_min = np.argmin(np.array(mse_arr))\n",
    "\n",
    "    # plot of the lambda values and their mse\n",
    "    plt.figure()\n",
    "    plt.semilogx(lam_arr, mse_arr)\n",
    "\n",
    "    # return the optimal lambda value\n",
    "    return lam_arr[lambda_idx_min]\n",
    "\n",
    "# call the function to choose the lambda for Ridge and Lasso\n",
    "lam_ridge = choose_hyper_param(X_train_n, y_train_n, X_train_v, y_train_v, True)\n",
    "lam_lasso = choose_hyper_param(X_train_n, y_train_n, X_train_v, y_train_v, False)\n",
    "\n",
    "print(\"Ridge lambda:\", lam_ridge)\n",
    "print(\"Lasso lambda:\", lam_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FAuX0uU5k9qD"
   },
   "source": [
    "### **Task 12**:\n",
    "Once you’ve obtained the optimal lambdas for Ridge and Lasso, train these models using these lambdas on the full training data. Then report\n",
    "the training and test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3259,
     "status": "ok",
     "timestamp": 1596436131187,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "VmwHESkg77zK",
    "outputId": "9bb9c1cf-1649-40e6-9162-2244525d9446"
   },
   "outputs": [],
   "source": [
    "# TODO: train the Ridge and Lasso models using the optimal parameters, and\n",
    "#       report their MSE\n",
    "###################################################\n",
    "##### YOUR CODE STARTS HERE #######################\n",
    "###################################################\n",
    "# Hints: train these models on the full training data\n",
    "mse_ridge_train = 0\n",
    "mse_ridge_test = 0\n",
    "mse_lasso_train = 0\n",
    "mse_lasso_test = 0\n",
    "###################################################\n",
    "##### YOUR CODE ENDS HERE #########################\n",
    "###################################################\n",
    "\n",
    "# Report the result\n",
    "print('For Ridge Regression with using degree %d polynomial expansion and lambda = %.4f' % (lam_ridge, lam_ridge))\n",
    "print('--------------------------------------------------------------------------------\\n')\n",
    "print('MSE (Training) = %.4f' % mse_ridge_train)\n",
    "print('MSE (Testing)  = %.4f' % mse_ridge_test)\n",
    "\n",
    "print('\\n\\nFor Lasso with using degree %d polynomial expansion and lambda = %.4f' % (lam_lasso, lam_lasso))\n",
    "print('---------------------------------------------------------------------\\n')\n",
    "print('MSE (Training) = %.4f' % mse_lasso_train)\n",
    "print('MSE (Testing)  = %.4f' % mse_lasso_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Os9tKKLd8gMU"
   },
   "source": [
    "## Optional: Try Larger Degrees using K-fold Cross Validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sfqRAlv1PBXi"
   },
   "source": [
    "### **Task 13**\n",
    "This is an optional task, which worths 5 bonus poitns.\n",
    "\n",
    "The task is to try basis expansions with higher degrees (up to degree 4) and find the degree that results the best performance. \n",
    "Instead of always using the same validation set, you should use k-fold cross validation to find the optimal hyperparameters. \n",
    "You should report the optimal hyperparameters (the basis expansion degree and the lambdas) and the MSE of the Ridge and Lasso when you apply the optimal hyperparameters. \n",
    "\n",
    "Hints: Use `KFold` to do this automatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kpwY7UtQ8l-0"
   },
   "outputs": [],
   "source": [
    "# KFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# TODO: Try using higher degree basis expansion. Find the degree that gives the minimal mse. \n",
    "###################################################\n",
    "##### YOUR CODE STARTS HERE #######################\n",
    "###################################################\n",
    "\n",
    "\n",
    "###################################################\n",
    "##### YOUR CODE ENDS HERE #########################\n",
    "###################################################"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP96ktvsOI4PiuW52tcNLjx",
   "collapsed_sections": [],
   "name": "Practical1_starter.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
